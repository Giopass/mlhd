{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#New code"
      ],
      "metadata": {
        "id": "pJb3p8-VE0BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "class ActivityRecognitionSystem:\n",
        "\n",
        "\n",
        "    def __init__(self, window_size=100, overlap=0.5):\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.step_size = int(window_size * (1 - overlap))\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "        # Activity mapping from the dataset documentation\n",
        "        self.activity_map = {\n",
        "            1: 'walking',\n",
        "            2: 'descending_stairs',\n",
        "            3: 'ascending_stairs',\n",
        "            4: 'driving',\n",
        "            77: 'clapping',\n",
        "            99: 'non_study_activity'\n",
        "        }\n",
        "\n",
        "    def download_and_extract_dataset(self):\n",
        "\n",
        "        base_path = '/content/accelerometry_data'\n",
        "        expected_path = f'{base_path}/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data'\n",
        "\n",
        "        # Check if dataset already exists\n",
        "        if os.path.exists(expected_path):\n",
        "            print(\"Dataset already exists, skipping download.\")\n",
        "            return True\n",
        "\n",
        "        print(\"Dataset not found. Downloading PhysioNet accelerometry dataset...\")\n",
        "\n",
        "        try:\n",
        "            # Create base directory\n",
        "            os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "            # Download URL\n",
        "            url = \"https://physionet.org/static/published-projects/accelerometry-walk-climb-drive/accelerometry-walk-climb-drive-1.0.0.zip\"\n",
        "            zip_path = f'{base_path}/dataset.zip'\n",
        "\n",
        "            # Download with progress\n",
        "            print(\"Downloading dataset (this may take a few minutes)...\")\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            downloaded_size = 0\n",
        "\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        downloaded_size += len(chunk)\n",
        "                        if total_size > 0:\n",
        "                            progress = (downloaded_size / total_size) * 100\n",
        "                            print(f\"\\rProgress: {progress:.1f}%\", end='')\n",
        "\n",
        "            print(\"\\nDownload complete! Extracting...\")\n",
        "\n",
        "            # Extract the zip file\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(base_path)\n",
        "\n",
        "            # Remove the zip file to save space\n",
        "            os.remove(zip_path)\n",
        "\n",
        "            print(\"Dataset extracted successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading dataset: {e}\")\n",
        "            print(\"Please download manually from: https://physionet.org/content/accelerometry-walk-climb-drive/1.0.0/\")\n",
        "            return False\n",
        "\n",
        "    def load_data(self):\n",
        "\n",
        "        # Try to download dataset if not present\n",
        "        if not self.download_and_extract_dataset():\n",
        "            return None\n",
        "\n",
        "        print(\"Loading accelerometry data from dataset...\")\n",
        "\n",
        "        # Based on the PhysioNet dataset structure\n",
        "        data_dir = '/content/accelerometry_data/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data'\n",
        "\n",
        "        if not os.path.exists(data_dir):\n",
        "            print(f\"Error: Directory {data_dir} not found even after download!\")\n",
        "            # Try alternative path in case of extraction differences\n",
        "            alt_path = '/content/accelerometry_data/accelerometry-walk-climb-drive-1.0.0/raw_accelerometry_data'\n",
        "            if os.path.exists(alt_path):\n",
        "                data_dir = alt_path\n",
        "                print(f\"Found dataset at alternative path: {data_dir}\")\n",
        "            else:\n",
        "                print(\"Dataset structure check:\")\n",
        "                if os.path.exists('/content/accelerometry_data'):\n",
        "                    for item in os.listdir('/content/accelerometry_data'):\n",
        "                        print(f\"  - {item}\")\n",
        "                return None\n",
        "\n",
        "        all_data = []\n",
        "        participant_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
        "\n",
        "        print(f\"Found {len(participant_files)} participant files\")\n",
        "\n",
        "        # Use all participants for better results\n",
        "        for file in participant_files:\n",
        "            file_path = os.path.join(data_dir, file)\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "                df['participant'] = file.replace('.csv', '')\n",
        "                all_data.append(df)\n",
        "                print(f\"Loaded {file}: {len(df)} samples\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "        if not all_data:\n",
        "            print(\"No data loaded!\")\n",
        "            return None\n",
        "\n",
        "        combined_data = pd.concat(all_data, ignore_index=True)\n",
        "        print(f\"Total samples loaded: {len(combined_data)}\")\n",
        "        print(f\"Columns: {list(combined_data.columns)}\")\n",
        "\n",
        "        return combined_data\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        \"\"\"\n",
        "        Preprocess the accelerometry data\n",
        "\n",
        "        Following guidelines: Pay attention to pre-processing phase (normalize data)\n",
        "        \"\"\"\n",
        "        print(\"Preprocessing data...\")\n",
        "\n",
        "        # Filter out clapping and non-study activities for main classification\n",
        "        # Focus on main activities: walking, stairs, driving\n",
        "        main_activities = data[data['activity'].isin([1, 2, 3, 4])].copy()\n",
        "\n",
        "        print(\"Activity distribution:\")\n",
        "        activity_counts = main_activities['activity'].value_counts().sort_index()\n",
        "        for activity_code, count in activity_counts.items():\n",
        "            activity_name = self.activity_map[activity_code]\n",
        "            percentage = (count / len(main_activities)) * 100\n",
        "            print(f\"  {activity_name}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "        # Extract accelerometer columns (12 features: 4 sensors √ó 3 axes)\n",
        "        accel_columns = ['lw_x', 'lw_y', 'lw_z',  # left wrist\n",
        "                        'lh_x', 'lh_y', 'lh_z',   # left hip\n",
        "                        'la_x', 'la_y', 'la_z',   # left ankle\n",
        "                        'ra_x', 'ra_y', 'ra_z']   # right ankle\n",
        "\n",
        "        # Check for missing values\n",
        "        missing_data = main_activities[accel_columns].isnull().sum().sum()\n",
        "        if missing_data > 0:\n",
        "            print(f\"Warning: {missing_data} missing values found. Filling with forward fill.\")\n",
        "            main_activities[accel_columns] = main_activities[accel_columns].fillna(method='ffill')\n",
        "\n",
        "        # Normalize the accelerometer data (critical preprocessing step)\n",
        "        print(\"Normalizing accelerometer data...\")\n",
        "        main_activities[accel_columns] = self.scaler.fit_transform(main_activities[accel_columns])\n",
        "\n",
        "        # Encode activity labels\n",
        "        main_activities['activity_encoded'] = self.label_encoder.fit_transform(main_activities['activity'])\n",
        "\n",
        "        return main_activities, accel_columns\n",
        "\n",
        "    def create_windows(self, data, accel_columns):\n",
        "        print(f\"Creating windows (size={self.window_size}, step={self.step_size})...\")\n",
        "\n",
        "        X_windows = []\n",
        "        y_windows = []\n",
        "        participant_ids = []\n",
        "\n",
        "        # Process each participant separately\n",
        "        for participant in data['participant'].unique():\n",
        "            participant_data = data[data['participant'] == participant].reset_index(drop=True)\n",
        "\n",
        "            # Create windows for this participant\n",
        "            for i in range(0, len(participant_data) - self.window_size + 1, self.step_size):\n",
        "                window_data = participant_data.iloc[i:i + self.window_size]\n",
        "\n",
        "                # Check if the window contains only one activity\n",
        "                if window_data['activity'].nunique() == 1:\n",
        "                    window_features = window_data[accel_columns].values\n",
        "                    activity_label = window_data['activity_encoded'].iloc[0]\n",
        "\n",
        "                    X_windows.append(window_features)\n",
        "                    y_windows.append(activity_label)\n",
        "                    participant_ids.append(participant)\n",
        "\n",
        "        X = np.array(X_windows)\n",
        "        y = np.array(y_windows)\n",
        "        participant_ids = np.array(participant_ids)\n",
        "\n",
        "        print(f\"Created {len(X)} windows from {len(np.unique(participant_ids))} participants\")\n",
        "        print(f\"Window shape: {X.shape}\")\n",
        "\n",
        "        # RETURN THREE VALUES\n",
        "        return X, y, participant_ids\n",
        "\n",
        "    def apply_data_balancing(self, X, y, balancing_method='smote'):\n",
        "\n",
        "        print(f\"\\nApplying {balancing_method} balancing...\")\n",
        "\n",
        "        # Print original distribution\n",
        "        original_counts = Counter(y)\n",
        "        print(\"Original class distribution:\")\n",
        "        for class_idx, count in sorted(original_counts.items()):\n",
        "            activity_name = self.activity_map[self.label_encoder.classes_[class_idx]]\n",
        "            percentage = (count / len(y)) * 100\n",
        "            print(f\"  {activity_name}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "        if balancing_method == 'smote':\n",
        "            return self._apply_smote(X, y)\n",
        "        elif balancing_method == 'undersampling':\n",
        "            return self._apply_undersampling(X, y)\n",
        "        elif balancing_method == 'oversampling':\n",
        "            return self._apply_oversampling(X, y)\n",
        "        elif balancing_method == 'class_weights':\n",
        "            return self._compute_class_weights(y), X, y\n",
        "        else:\n",
        "            raise ValueError(\"balancing_method must be 'smote', 'undersampling', 'oversampling', or 'class_weights'\")\n",
        "\n",
        "    def _apply_smote(self, X, y):\n",
        "\n",
        "        print(\"Applying lightweight SMOTE\")\n",
        "\n",
        "        class_counts = Counter(y)\n",
        "\n",
        "        # Set a reasonable target count (median * 1.2 to be more conservative)\n",
        "        target_count = int(np.median(list(class_counts.values())) * 1.2)\n",
        "        print(f\"Target samples per class: {target_count}\")\n",
        "\n",
        "        X_balanced = []\n",
        "        y_balanced = []\n",
        "\n",
        "        for class_idx in np.unique(y):\n",
        "            class_mask = y == class_idx\n",
        "            class_samples = X[class_mask]\n",
        "            class_labels = y[class_mask]\n",
        "\n",
        "            current_count = len(class_samples)\n",
        "            activity_name = self.activity_map[self.label_encoder.classes_[class_idx]]\n",
        "\n",
        "            if current_count < target_count:\n",
        "                # Minority class - oversample with SMOTE but limit synthetic data to 25%\n",
        "                samples_needed = target_count - current_count\n",
        "\n",
        "                # Calculate maximum synthetic samples (25% of final count)\n",
        "                max_synthetic = int(target_count * 0.25)\n",
        "\n",
        "                # Use the minimum of what's needed and what's allowed (25% rule)\n",
        "                synthetic_to_create = min(samples_needed, max_synthetic)\n",
        "\n",
        "                # If we can't reach target with 25% synthetic, adjust target downward\n",
        "                if synthetic_to_create < samples_needed:\n",
        "                    adjusted_target = current_count + synthetic_to_create\n",
        "                    print(f\"  {activity_name}: Adjusting target from {target_count} to {adjusted_target} due to 25% synthetic limit\")\n",
        "                    target_count_for_class = adjusted_target\n",
        "                else:\n",
        "                    target_count_for_class = target_count\n",
        "\n",
        "                # Create synthetic samples\n",
        "                synthetic_samples = []\n",
        "                for _ in range(synthetic_to_create):\n",
        "                    # Pick two random samples from the same class\n",
        "                    idx1, idx2 = np.random.choice(current_count, 2, replace=False if current_count > 1 else True)\n",
        "                    sample1 = class_samples[idx1]\n",
        "                    sample2 = class_samples[idx2]\n",
        "\n",
        "                    # Linear interpolation with random alpha\n",
        "                    alpha = np.random.random()\n",
        "                    synthetic_sample = sample1 + alpha * (sample2 - sample1)\n",
        "                    synthetic_samples.append(synthetic_sample)\n",
        "\n",
        "                # Add original samples\n",
        "                X_balanced.extend(class_samples)\n",
        "                y_balanced.extend(class_labels)\n",
        "\n",
        "                # Add synthetic samples\n",
        "                if synthetic_samples:\n",
        "                    X_balanced.extend(synthetic_samples)\n",
        "                    y_balanced.extend([class_idx] * synthetic_to_create)\n",
        "\n",
        "                synthetic_percentage = (synthetic_to_create / target_count_for_class) * 100\n",
        "                print(f\"  {activity_name}: {current_count} ‚Üí {target_count_for_class} (+{synthetic_to_create} synthetic, {synthetic_percentage:.1f}%)\")\n",
        "\n",
        "            elif current_count > target_count:\n",
        "                # Majority class - undersample\n",
        "                indices = np.random.choice(current_count, target_count, replace=False)\n",
        "                sampled_X = class_samples[indices]\n",
        "                sampled_y = class_labels[indices]\n",
        "\n",
        "                X_balanced.extend(sampled_X)\n",
        "                y_balanced.extend(sampled_y)\n",
        "\n",
        "                print(f\"  {activity_name}: {current_count} ‚Üí {target_count} (-{current_count - target_count} removed)\")\n",
        "\n",
        "            else:\n",
        "                # Already at target\n",
        "                X_balanced.extend(class_samples)\n",
        "                y_balanced.extend(class_labels)\n",
        "                print(f\"  {activity_name}: {current_count} samples (no change)\")\n",
        "\n",
        "        X_balanced = np.array(X_balanced)\n",
        "        y_balanced = np.array(y_balanced)\n",
        "\n",
        "        # Shuffle\n",
        "        indices = np.random.permutation(len(X_balanced))\n",
        "        return None, X_balanced[indices], y_balanced[indices]\n",
        "\n",
        "    def _apply_undersampling(self, X, y):\n",
        "\n",
        "        print(\"Applying random undersampling...\")\n",
        "\n",
        "        class_counts = Counter(y)\n",
        "        min_count = min(class_counts.values())\n",
        "\n",
        "        X_balanced = []\n",
        "        y_balanced = []\n",
        "\n",
        "        for class_idx in np.unique(y):\n",
        "            class_mask = y == class_idx\n",
        "            class_samples = X[class_mask]\n",
        "            class_labels = y[class_mask]\n",
        "\n",
        "            if len(class_samples) > min_count:\n",
        "                indices = np.random.choice(len(class_samples), min_count, replace=False)\n",
        "                sampled_X = class_samples[indices]\n",
        "                sampled_y = class_labels[indices]\n",
        "            else:\n",
        "                sampled_X = class_samples\n",
        "                sampled_y = class_labels\n",
        "\n",
        "            X_balanced.append(sampled_X)\n",
        "            y_balanced.append(sampled_y)\n",
        "\n",
        "            activity_name = self.activity_map[self.label_encoder.classes_[class_idx]]\n",
        "            print(f\"  {activity_name}: {len(class_samples)} ‚Üí {len(sampled_X)} samples\")\n",
        "\n",
        "        X_balanced = np.vstack(X_balanced)\n",
        "        y_balanced = np.concatenate(y_balanced)\n",
        "\n",
        "        indices = np.random.permutation(len(X_balanced))\n",
        "        return None, X_balanced[indices], y_balanced[indices]\n",
        "\n",
        "    def _apply_oversampling(self, X, y):\n",
        "\n",
        "        print(\"Applying random oversampling...\")\n",
        "\n",
        "        class_counts = Counter(y)\n",
        "        max_count = max(class_counts.values())\n",
        "\n",
        "        X_balanced = []\n",
        "        y_balanced = []\n",
        "\n",
        "        for class_idx in np.unique(y):\n",
        "            class_mask = y == class_idx\n",
        "            class_samples = X[class_mask]\n",
        "            class_labels = y[class_mask]\n",
        "\n",
        "            if len(class_samples) < max_count:\n",
        "                indices = np.random.choice(len(class_samples), max_count, replace=True)\n",
        "                sampled_X = class_samples[indices]\n",
        "                sampled_y = class_labels[indices]\n",
        "            else:\n",
        "                sampled_X = class_samples\n",
        "                sampled_y = class_labels\n",
        "\n",
        "            X_balanced.append(sampled_X)\n",
        "            y_balanced.append(sampled_y)\n",
        "\n",
        "            activity_name = self.activity_map[self.label_encoder.classes_[class_idx]]\n",
        "            print(f\"  {activity_name}: {len(class_samples)} ‚Üí {len(sampled_X)} samples\")\n",
        "\n",
        "        X_balanced = np.vstack(X_balanced)\n",
        "        y_balanced = np.concatenate(y_balanced)\n",
        "\n",
        "        indices = np.random.permutation(len(X_balanced))\n",
        "        return None, X_balanced[indices], y_balanced[indices]\n",
        "\n",
        "    def _compute_class_weights(self, y):\n",
        "\n",
        "        print(\"Computing class weights for balanced training...\")\n",
        "\n",
        "        classes = np.unique(y)\n",
        "        class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "        class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "        print(\"Class weights:\")\n",
        "        for class_idx, weight in class_weight_dict.items():\n",
        "            activity_name = self.activity_map[self.label_encoder.classes_[class_idx]]\n",
        "            print(f\"  {activity_name}: {weight:.3f}\")\n",
        "\n",
        "        return class_weight_dict\n",
        "\n",
        "    def create_cnn_model(self, input_shape, num_classes):\n",
        "\n",
        "        print(\"Creating CNN model with anti-overfitting measures...\")\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            # Reduced complexity - fewer filters\n",
        "            tf.keras.layers.Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=input_shape),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.Dropout(0.4),  # Increased dropout\n",
        "\n",
        "            tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.Dropout(0.5),  # Increased dropout\n",
        "\n",
        "            tf.keras.layers.GlobalMaxPooling1D(),\n",
        "            tf.keras.layers.Dropout(0.6),  # High dropout before dense layers\n",
        "\n",
        "            # Smaller dense layers\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.7),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_lstm_model(self, input_shape, num_classes):\n",
        "\n",
        "        print(\"Creating LSTM model with anti-overfitting measures...\")\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            # Smaller LSTM units\n",
        "            tf.keras.layers.LSTM(32, return_sequences=True, input_shape=input_shape,\n",
        "                               dropout=0.3, recurrent_dropout=0.3),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "            tf.keras.layers.LSTM(16, return_sequences=False,\n",
        "                               dropout=0.4, recurrent_dropout=0.4),\n",
        "            tf.keras.layers.Dropout(0.6),\n",
        "\n",
        "            # Smaller dense layer\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.7),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_hybrid_model(self, input_shape, num_classes):\n",
        "\n",
        "        print(\"Creating Hybrid model with anti-overfitting measures...\")\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            # Smaller CNN part\n",
        "            tf.keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "            # Smaller LSTM part\n",
        "            tf.keras.layers.LSTM(16, return_sequences=False, dropout=0.3, recurrent_dropout=0.3),\n",
        "            tf.keras.layers.Dropout(0.6),\n",
        "\n",
        "            # Smaller dense layer\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.7),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, X_train, X_test, y_train, y_test, model_type='cnn', class_weights=None):\n",
        "\n",
        "        print(f\"\\nTraining {model_type.upper()} model...\")\n",
        "\n",
        "        num_classes = len(np.unique(y_train)) # Get the actual number of unique classes in the training data\n",
        "\n",
        "        if model_type == 'cnn':\n",
        "            self.model = self.create_cnn_model(X_train.shape[1:], num_classes)\n",
        "        elif model_type == 'lstm':\n",
        "            self.model = self.create_lstm_model(X_train.shape[1:], num_classes)\n",
        "        elif model_type == 'hybrid':\n",
        "            self.model = self.create_hybrid_model(X_train.shape[1:], num_classes)\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
        "        ]\n",
        "\n",
        "        start_time = time.time()\n",
        "        start_memory = psutil.Process().memory_info().rss / 1024 / 1024\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=30,  # Reduced for faster testing\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        end_memory = psutil.Process().memory_info().rss / 1024 / 1024\n",
        "        memory_usage = end_memory - start_memory\n",
        "\n",
        "        return training_time, memory_usage\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test):\n",
        "\n",
        "        y_pred_proba = self.model.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        return accuracy, y_pred, y_pred_proba\n",
        "\n",
        "def participant_aware_split(X, y, participant_ids, test_size=0.2, random_state=42):\n",
        "\n",
        "    print(\"Performing participant-aware train/test split...\")\n",
        "\n",
        "    unique_participants = np.unique(participant_ids)\n",
        "    np.random.seed(random_state)\n",
        "    np.random.shuffle(unique_participants)\n",
        "\n",
        "    # Split participants, not windows\n",
        "    n_test_participants = max(1, int(len(unique_participants) * test_size))\n",
        "    test_participants = set(unique_participants[:n_test_participants])\n",
        "    train_participants = set(unique_participants[n_test_participants:])\n",
        "\n",
        "    print(f\"Train participants: {len(train_participants)}\")\n",
        "    print(f\"Test participants: {len(test_participants)}\")\n",
        "\n",
        "    # Create masks\n",
        "    train_mask = np.array([p in train_participants for p in participant_ids])\n",
        "    test_mask = np.array([p in test_participants for p in participant_ids])\n",
        "\n",
        "    X_train, X_test = X[train_mask], X[test_mask]\n",
        "    y_train, y_test = y[train_mask], y[test_mask]\n",
        "\n",
        "    print(f\"Train windows: {len(X_train)}\")\n",
        "    print(f\"Test windows: {len(X_test)}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def compare_architectures():\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"ACTIVITY RECOGNITION - FAST MODE (30% DATASET)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # Initialize system with smaller windows for speed\n",
        "    system = ActivityRecognitionSystem(window_size=25, overlap=0.1)  # Much smaller windows, minimal overlap\n",
        "\n",
        "    print(\"Step 1/6: Loading dataset...\")\n",
        "    step_start = time.time()\n",
        "    raw_data = system.load_data()\n",
        "    if raw_data is None:\n",
        "        return\n",
        "    print(f\"‚úì Dataset loaded in {time.time() - step_start:.1f}s\")\n",
        "\n",
        "    print(\"\\nStep 2/6: Preprocessing data...\")\n",
        "    step_start = time.time()\n",
        "    processed_data, accel_columns = system.preprocess_data(raw_data)\n",
        "    print(f\"‚úì Preprocessing completed in {time.time() - step_start:.1f}s\")\n",
        "\n",
        "    print(\"\\nStep 3/6: Creating windows...\")\n",
        "    step_start = time.time()\n",
        "    X, y, participant_ids = system.create_windows(processed_data, accel_columns)\n",
        "    print(f\"‚úì Window creation completed in {time.time() - step_start:.1f}s\")\n",
        "\n",
        "    print(\"\\nStep 4/6: Splitting data...\")\n",
        "    step_start = time.time()\n",
        "    X_train_orig, X_test, y_train_orig, y_test = participant_aware_split(\n",
        "        X, y, participant_ids, test_size=0.2, random_state=42)\n",
        "    print(f\"‚úì Data split completed in {time.time() - step_start:.1f}s\")\n",
        "\n",
        "    print(f\"\\nDataset sizes:\")\n",
        "    print(f\"  Training: {X_train_orig.shape[0]} samples\")\n",
        "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
        "\n",
        "    balancing_methods = ['smote', 'class_weights']\n",
        "    models_to_compare = ['cnn', 'hybrid']\n",
        "    total_combinations = len(balancing_methods) * len(models_to_compare)\n",
        "    current_combination = 0\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    print(f\"\\nStep 5/6: Training {total_combinations} model combinations...\")\n",
        "\n",
        "    for balancing_method in balancing_methods:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"BALANCING METHOD: {balancing_method.upper()}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        step_start = time.time()\n",
        "        if balancing_method == 'class_weights':\n",
        "            class_weights, X_train, y_train = system.apply_data_balancing(\n",
        "                X_train_orig, y_train_orig, balancing_method)\n",
        "        else:\n",
        "            _, X_train, y_train = system.apply_data_balancing(\n",
        "                X_train_orig, y_train_orig, balancing_method)\n",
        "            class_weights = None\n",
        "        print(f\"‚úì Balancing completed in {time.time() - step_start:.1f}s\")\n",
        "\n",
        "        method_results = {}\n",
        "\n",
        "        for model_type in models_to_compare:\n",
        "            current_combination += 1\n",
        "            print(f\"\\n[{current_combination}/{total_combinations}] Training {model_type.upper()} with {balancing_method}\")\n",
        "\n",
        "            model_system = ActivityRecognitionSystem(window_size=25, overlap=0.1)\n",
        "            model_system.scaler = system.scaler\n",
        "            model_system.label_encoder = system.label_encoder\n",
        "\n",
        "            model_start_time = time.time()\n",
        "            training_time, memory_usage = model_system.train_model(\n",
        "                X_train, X_test, y_train, y_test,\n",
        "                model_type=model_type, class_weights=class_weights)\n",
        "\n",
        "            accuracy, y_pred, y_pred_proba = model_system.evaluate_model(X_test, y_test)\n",
        "\n",
        "            method_results[model_type] = {\n",
        "                'accuracy': accuracy,\n",
        "                'training_time': training_time,\n",
        "                'memory_usage': memory_usage\n",
        "            }\n",
        "\n",
        "            model_total_time = time.time() - model_start_time\n",
        "            print(f\"‚úì {model_type.upper()} completed in {model_total_time:.1f}s - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            # Quick overfitting check\n",
        "            if len(X_train) > 500:\n",
        "                sample_size = min(500, len(X_train))\n",
        "                train_acc = model_system.model.evaluate(X_train[:sample_size], y_train[:sample_size], verbose=0)[1]\n",
        "                if train_acc - accuracy > 0.15:\n",
        "                    print(f\"‚ö†Ô∏è  Overfitting detected (train: {train_acc:.3f}, test: {accuracy:.3f})\")\n",
        "\n",
        "        all_results[balancing_method] = method_results\n",
        "\n",
        "    print(f\"\\nStep 6/6: Generating results...\")\n",
        "    step_start = time.time()\n",
        "\n",
        "    # Results summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RESULTS SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    comparison_data = []\n",
        "    for balancing_method in balancing_methods:\n",
        "        for model_type in models_to_compare:\n",
        "            result = all_results[balancing_method][model_type]\n",
        "            comparison_data.append({\n",
        "                'Balancing': balancing_method,\n",
        "                'Model': model_type,\n",
        "                'Accuracy': result['accuracy'],\n",
        "                'Training Time (s)': result['training_time'],\n",
        "                'Memory Usage (MB)': result['memory_usage']\n",
        "            })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "    # Best combination\n",
        "    best_idx = comparison_df['Accuracy'].idxmax()\n",
        "    best_combination = comparison_df.iloc[best_idx]\n",
        "\n",
        "    total_time = time.time() - total_start_time\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"BEST COMBINATION:\")\n",
        "    print(f\"Balancing: {best_combination['Balancing']}\")\n",
        "    print(f\"Model: {best_combination['Model']}\")\n",
        "    print(f\"Accuracy: {best_combination['Accuracy']:.4f}\")\n",
        "    print(f\"Total Runtime: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(f\" Results generated in {time.time() - step_start:.1f}s\")\n",
        "    print(f\" ANALYSIS COMPLETE! Total time: {total_time:.1f}s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    compare_architectures()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmAGkg2-Bu9h",
        "outputId": "916da9a5-a326-4d99-a85e-92fbf27c4640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ACTIVITY RECOGNITION - FAST MODE (30% DATASET)\n",
            "============================================================\n",
            "Step 1/6: Loading dataset...\n",
            "Dataset already exists, skipping download.\n",
            "Loading accelerometry data from dataset...\n",
            "Found 32 participant files\n",
            "Loaded id079c763c.csv: 284300 samples\n",
            "Loaded idf540d82b.csv: 243000 samples\n",
            "Loaded idecc9265e.csv: 346600 samples\n",
            "Loaded idc735fc09.csv: 345900 samples\n",
            "Loaded id4ea159a8.csv: 267900 samples\n",
            "Loaded idf1ce9a0f.csv: 328600 samples\n",
            "Loaded id7c20ee7a.csv: 322300 samples\n",
            "Loaded id34e056c8.csv: 341700 samples\n",
            "Loaded id8af5374b.csv: 292700 samples\n",
            "Loaded idbae5a811.csv: 422400 samples\n",
            "Loaded id5308a7d6.csv: 323700 samples\n",
            "Loaded idff99de96.csv: 330900 samples\n",
            "Loaded id9603e9c3.csv: 111300 samples\n",
            "Loaded id82b9735c.csv: 304800 samples\n",
            "Loaded id650857ca.csv: 309000 samples\n",
            "Loaded ida61e8ddf.csv: 123200 samples\n",
            "Loaded idabd0c53c.csv: 94200 samples\n",
            "Loaded id1c7e64ad.csv: 342700 samples\n",
            "Loaded idc91a49d0.csv: 310100 samples\n",
            "Loaded id1f372081.csv: 166100 samples\n",
            "Loaded idfc5f05e4.csv: 285200 samples\n",
            "Loaded id5993bf4a.csv: 253700 samples\n",
            "Loaded id8e66893c.csv: 333800 samples\n",
            "Loaded id00b70b13.csv: 303300 samples\n",
            "Loaded id37a54bbf.csv: 331500 samples\n",
            "Loaded id3e3e50c7.csv: 280500 samples\n",
            "Loaded id1165e00c.csv: 325800 samples\n",
            "Loaded idb221f542.csv: 291000 samples\n",
            "Loaded id86237981.csv: 292700 samples\n",
            "Loaded id687ab496.csv: 325800 samples\n",
            "Loaded idf5e3678b.csv: 446600 samples\n",
            "Loaded idd80ac2b4.csv: 100800 samples\n",
            "Total samples loaded: 9182100\n",
            "Columns: ['activity', 'time_s', 'lw_x', 'lw_y', 'lw_z', 'lh_x', 'lh_y', 'lh_z', 'la_x', 'la_y', 'la_z', 'ra_x', 'ra_y', 'ra_z', 'participant']\n",
            "‚úì Dataset loaded in 22.3s\n",
            "\n",
            "Step 2/6: Preprocessing data...\n",
            "Preprocessing data...\n",
            "Activity distribution:\n",
            "  walking: 1576845 samples (26.9%)\n",
            "  descending_stairs: 267538 samples (4.6%)\n",
            "  ascending_stairs: 279907 samples (4.8%)\n",
            "  driving: 3741501 samples (63.8%)\n",
            "Normalizing accelerometer data...\n",
            "‚úì Preprocessing completed in 3.2s\n",
            "\n",
            "Step 3/6: Creating windows...\n",
            "Creating windows (size=25, step=22)...\n",
            "Created 266012 windows from 32 participants\n",
            "Window shape: (266012, 25, 12)\n",
            "‚úì Window creation completed in 173.6s\n",
            "\n",
            "Step 4/6: Splitting data...\n",
            "Performing participant-aware train/test split...\n",
            "Train participants: 26\n",
            "Test participants: 6\n",
            "Train windows: 210420\n",
            "Test windows: 55592\n",
            "‚úì Data split completed in 0.8s\n",
            "\n",
            "Dataset sizes:\n",
            "  Training: 210420 samples\n",
            "  Test: 55592 samples\n",
            "\n",
            "Step 5/6: Training 4 model combinations...\n",
            "\n",
            "==================================================\n",
            "BALANCING METHOD: SMOTE\n",
            "==================================================\n",
            "\n",
            "Applying smote balancing...\n",
            "Original class distribution:\n",
            "  walking: 58210 samples (27.7%)\n",
            "  descending_stairs: 9609 samples (4.6%)\n",
            "  ascending_stairs: 10149 samples (4.8%)\n",
            "  driving: 132452 samples (62.9%)\n",
            "Applying lightweight SMOTE with limited synthetic data (max 25% synthetic)...\n",
            "Target samples per class: 41015\n",
            "  walking: 58210 ‚Üí 41015 (-17195 removed)\n",
            "  descending_stairs: Adjusting target from 41015 to 19862 due to 25% synthetic limit\n",
            "  descending_stairs: 9609 ‚Üí 19862 (+10253 synthetic, 51.6%)\n",
            "  ascending_stairs: Adjusting target from 41015 to 20402 due to 25% synthetic limit\n",
            "  ascending_stairs: 10149 ‚Üí 20402 (+10253 synthetic, 50.3%)\n",
            "  driving: 132452 ‚Üí 41015 (-91437 removed)\n",
            "‚úì Balancing completed in 5.1s\n",
            "\n",
            "[1/4] Training CNN with smote\n",
            "\n",
            "Training CNN model...\n",
            "Creating CNN model with anti-overfitting measures...\n",
            "\u001b[1m1738/1738\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
            "‚úì CNN completed in 372.4s - Accuracy: 0.9162\n",
            "\n",
            "[2/4] Training HYBRID with smote\n",
            "\n",
            "Training HYBRID model...\n",
            "Creating Hybrid model with anti-overfitting measures...\n",
            "\u001b[1m1738/1738\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
            "‚úì HYBRID completed in 769.1s - Accuracy: 0.8034\n",
            "\n",
            "==================================================\n",
            "BALANCING METHOD: CLASS_WEIGHTS\n",
            "==================================================\n",
            "\n",
            "Applying class_weights balancing...\n",
            "Original class distribution:\n",
            "  walking: 58210 samples (27.7%)\n",
            "  descending_stairs: 9609 samples (4.6%)\n",
            "  ascending_stairs: 10149 samples (4.8%)\n",
            "  driving: 132452 samples (62.9%)\n",
            "Computing class weights for balanced training...\n",
            "Class weights:\n",
            "  walking: 0.904\n",
            "  descending_stairs: 5.475\n",
            "  ascending_stairs: 5.183\n",
            "  driving: 0.397\n",
            "‚úì Balancing completed in 0.1s\n",
            "\n",
            "[3/4] Training CNN with class_weights\n",
            "\n",
            "Training CNN model...\n",
            "Creating CNN model with anti-overfitting measures...\n",
            "\u001b[1m1738/1738\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
            "‚úì CNN completed in 602.9s - Accuracy: 0.7086\n",
            "\n",
            "[4/4] Training HYBRID with class_weights\n",
            "\n",
            "Training HYBRID model...\n",
            "Creating Hybrid model with anti-overfitting measures...\n",
            "\u001b[1m1738/1738\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
            "‚úì HYBRID completed in 1394.1s - Accuracy: 0.8831\n",
            "\n",
            "Step 6/6: Generating results...\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "    Balancing  Model  Accuracy  Training Time (s)  Memory Usage (MB)\n",
            "        smote    cnn    0.9162           366.8696          -545.1250\n",
            "        smote hybrid    0.8034           762.6575            41.3828\n",
            "class_weights    cnn    0.7086           597.5602            54.8008\n",
            "class_weights hybrid    0.8831          1388.2100            47.6289\n",
            "\n",
            "============================================================\n",
            "BEST COMBINATION:\n",
            "Balancing: smote\n",
            "Model: cnn\n",
            "Accuracy: 0.9162\n",
            "Total Runtime: 3344.1 seconds (55.7 minutes)\n",
            "============================================================\n",
            "‚úì Results generated in 0.0s\n",
            "üéâ ANALYSIS COMPLETE! Total time: 3344.1s\n"
          ]
        }
      ]
    }
  ]
}